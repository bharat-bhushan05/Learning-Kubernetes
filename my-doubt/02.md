```
PS C:\Users\Bharat Bhushan\Documents\k8project\Slab.ai\manifest> kubectl get pods
NAME                                          READY   STATUS    RESTARTS   AGE
payment-service-deployment-76bc7f65fd-2wnzj   0/1     Pending   0          24s
payment-service-deployment-76bc7f65fd-6nkhh   0/1     Pending   0          24s
payment-service-deployment-76bc7f65fd-nstw8   0/1     Pending   0          24s

PS C:\Users\Bharat Bhushan\Documents\k8project\Slab.ai\manifest> kubectl describe pod payment-service-deployment-76bc7f65fd-2wnzj
Name:             payment-service-deployment-76bc7f65fd-2wnzj
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=payment-service
                  pod-template-hash=76bc7f65fd
Annotations:      <none>
Status:           Pending
IP:
IPs:              <none>
Controlled By:    ReplicaSet/payment-service-deployment-76bc7f65fd
Containers:
  payment-service:
    Image:        bharatbhushan05/payment-service
    Port:         5000/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pkmxw (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
  kube-api-access-pkmxw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  83s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) had untolerated taint {node.kubernetes.io/unreachable: }. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.

```
## Solution:
The error message you are seeing indicates that your Kubernetes pod is failing to be scheduled due to the following reasons:

1. **Tainted Nodes**: The nodes in your cluster have taints that prevent the pod from being scheduled. Specifically, the error mentions two types of taints:
   - `node-role.kubernetes.io/control-plane`: This taint is typically applied to the control plane nodes (master nodes) to ensure that regular application pods are not scheduled on them. 
   - `node.kubernetes.io/unreachable`: This taint indicates that the node is unreachable, meaning it's either down or not communicating with the Kubernetes control plane.

2. **Preemption**: Preemption is the process where Kubernetes can terminate lower-priority pods to make space for higher-priority pods. In your case, it seems that preemption is not helpful, meaning no preemptible pods were available to be terminated to make room for your pod.

### How to resolve this issue:

#### 1. **Check Node Status**:
   - Run the following command to check the status of your nodes:
     ```bash
     kubectl get nodes
     ```
   - Look for any nodes marked as `NotReady` or `Unreachable`. These nodes may need attention (e.g., they might need to be restarted or have their network connectivity fixed).

#### 2. **Tolerate the Taints**:
   - If your pod needs to be scheduled on a node with the `node-role.kubernetes.io/control-plane` taint, you will need to add a **toleration** to your pod configuration to allow it to be scheduled on the control plane node.
   - You can add the following toleration to your pod's YAML file:
     ```yaml
     tolerations:
     - key: "node-role.kubernetes.io/control-plane"
       operator: "Exists"
       effect: "NoSchedule"
     ```

#### 3. **Check the Node with `unreachable` Taint**:
   - Nodes with the `unreachable` taint are often down or disconnected. If you have such nodes, ensure they are reachable or remove them from your cluster if they are no longer needed.
   - You can remove the unreachable taint manually by running:
     ```bash
     kubectl taint nodes <node-name> node.kubernetes.io/unreachable-
     ```

#### 4. **Check Resource Availability**:
   - Ensure that your nodes have enough resources (CPU, memory, etc.) to schedule the pod. You can describe the pod and check its resource requests and limits:
     ```bash
     kubectl describe pod <pod-name>
     ```
   - If your pod requests too many resources, it might not be able to fit onto the available nodes.

#### 5. **Scale Your Cluster**:
   - If you are running on a small cluster or the available nodes are full, you might need to add more nodes to your cluster.

#### 6. **Review Pod Priority and Preemption**:
   - If preemption is not helping, ensure that your pod has a higher priority or is appropriately configured to preempt other pods. You can set the priority class for the pod:
     ```yaml
     apiVersion: scheduling.k8s.io/v1
     kind: PriorityClass
     metadata:
       name: high-priority
     value: 1000000
     globalDefault: false
     description: "High priority class"
     ```
     Then, assign it to your pod:
     ```yaml
     priorityClassName: high-priority
     ```

### Summary of Steps:
1. Check node status and ensure they are ready.
2. Add tolerations for `control-plane` taint if needed.
3. Address the `unreachable` taint issue.
4. Ensure sufficient resources on your nodes.
5. Consider scaling your cluster if needed.
6. Review pod priority and preemption settings.

If you still encounter issues, let me know and we can dive deeper!
